{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52824b89-532a-4e54-87e9-1410813cd39e",
      "metadata": {
        "id": "52824b89-532a-4e54-87e9-1410813cd39e"
      },
      "source": [
        "# Lab | Chains in LangChain\n",
        "\n",
        "## Outline\n",
        "\n",
        "* LLMChain\n",
        "* Sequential Chains\n",
        "  * SimpleSequentialChain\n",
        "  * SequentialChain\n",
        "* Router Chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elbnlS9ZOKNu",
        "outputId": "ddd2ea01-4cfc-4926-eb81-372da0203428"
      },
      "id": "elbnlS9ZOKNu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "541eb2f1",
      "metadata": {
        "id": "541eb2f1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
        "outputId": "31cfad3a-c272-4834-de32-f5b9ee6cfed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and set as environment variable.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# Retrieve the API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "# Set it as an environment variable, which LangChain often uses by default\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "print(\"OpenAI API key loaded and set as environment variable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84e441b",
      "metadata": {
        "tags": [],
        "id": "b84e441b"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
      "metadata": {
        "tags": [],
        "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a09c35",
      "metadata": {
        "tags": [],
        "id": "b7a09c35",
        "outputId": "2a680e8c-16f1-46d2-9510-2fc596fcfc10"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\r\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Product  \\\n",
              "0       Queen Size Sheet Set   \n",
              "1     Waterproof Phone Pouch   \n",
              "2        Luxury Air Mattress   \n",
              "3             Pillows Insert   \n",
              "4  Milk Frother Handheld\\r\\n   \n",
              "\n",
              "                                              Review  \n",
              "0  I ordered a king size set. My only criticism w...  \n",
              "1  I loved the waterproof sac, although the openi...  \n",
              "2  This mattress had a small hole in the top of i...  \n",
              "3  This is the best throw pillow fillers on Amazo...  \n",
              "4  ¬†I loved this product. But they only seem to l...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b940ce7c",
      "metadata": {
        "id": "b940ce7c"
      },
      "source": [
        "## LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "427e1119",
      "metadata": {
        "id": "427e1119",
        "outputId": "fc5dc231-961c-41b5-be14-c712aa48f496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\appul\\anaconda3\\lib\\site-packages (1.0.7)\n",
            "Requirement already satisfied: langchain-openai in c:\\users\\appul\\anaconda3\\lib\\site-packages (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\appul\\anaconda3\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.4 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\appul\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.4->langchain) (2.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\appul\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.7.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\appul\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\appul\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\appul\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-openai) (2.7.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\appul\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\appul\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\appul\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e92dff22",
      "metadata": {
        "tags": [],
        "id": "e92dff22"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "943237a7",
      "metadata": {
        "id": "943237a7"
      },
      "outputs": [],
      "source": [
        "#Replace None by your own value and justify\n",
        "llm = ChatOpenAI(temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cdcdb42d",
      "metadata": {
        "id": "cdcdb42d"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template( \"Tell me about the product: {product_description}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d7abc20b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7abc20b",
        "outputId": "2ebe2b1d-15f7-469e-e2d8-1900f01f66bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-546483037.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ad44d1fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ad44d1fb",
        "outputId": "d680e9c2-7f80-4802-b83b-e0b0056c23cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1892552926.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(product)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A luxury air mattress is a high-quality, durable mattress that provides a comfortable and supportive sleeping surface. These mattresses are typically made with advanced materials and construction techniques to ensure optimal comfort and durability.\\n\\nLuxury air mattresses often feature multiple air chambers that can be adjusted to customize the firmness level to suit individual preferences. Many models also include additional features such as built-in pumps for easy inflation and deflation, plush pillow tops for extra comfort, and antimicrobial treatments to prevent mold and mildew growth.\\n\\nThese mattresses are ideal for use at home as a permanent bed or as a temporary sleeping solution for guests. They are also popular for outdoor use, such as camping or RV trips, as they are portable and easy to set up.\\n\\nOverall, a luxury air mattress offers a luxurious sleeping experience with the convenience and versatility of an inflatable bed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "product = \"Luxury Air Mattress\"\n",
        "chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b03469",
      "metadata": {
        "id": "69b03469"
      },
      "source": [
        "## SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "febee243",
      "metadata": {
        "id": "febee243"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2f31aa8a",
      "metadata": {
        "id": "2f31aa8a"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
        "        model=\"gpt-3.5-turbo\", temperature=0.9)\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"As an expert in product marketing, analyze the following product description:\\n\\n{product_description}\\n\\n\"\n",
        "    \"Based on this, suggest 3 concise, catchy, and benefit-oriented taglines. \"\n",
        "    \"Format them as a numbered list.\"\n",
        ")\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3f5d5b76",
      "metadata": {
        "id": "3f5d5b76"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Based on these taglines for a product:\\n\\n{taglines_from_first_chain}\\n\\n\"\n",
        "    \"Suggest a creative, short social media post (e.g., for Instagram or X) \"\n",
        "    \"that incorporates one of these taglines. Include relevant emojis and hashtags.\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6c1eb2c4",
      "metadata": {
        "id": "6c1eb2c4"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "78458efe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "78458efe",
        "outputId": "050ca979-7d4e-44d1-b125-1b8508e7372d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m1. \"Experience the Ultimate Comfort: Luxury Air Mattress\"\n",
            "2. \"Elevate Your Sleep: Luxury Air Mattress\"\n",
            "3. \"Rest in Style: Luxury Air Mattress\"\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\"Upgrade your sleep game with our new Luxury Air Mattress! üí§‚ú® Experience the ultimate comfort and elevate your rest to a whole new level. #LuxurySleep #ComfyNights\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Upgrade your sleep game with our new Luxury Air Mattress! üí§‚ú® Experience the ultimate comfort and elevate your rest to a whole new level. #LuxurySleep #ComfyNights\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "overall_simple_chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd59bda-9d02-44e7-b3d6-2bec61b99d8f",
      "metadata": {
        "id": "0dd59bda-9d02-44e7-b3d6-2bec61b99d8f"
      },
      "source": [
        "**Repeat the above twice for different products**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5ce18c",
      "metadata": {
        "id": "7b5ce18c"
      },
      "source": [
        "## SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4c129ef6",
      "metadata": {
        "id": "4c129ef6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "016187ac",
      "metadata": {
        "id": "016187ac"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "  \"As experienced product testerr, give me a summary of the best reviews for the following product:\\n\\n{product_description}\\n\\n\"\n",
        "  \"The summary has 2 sentences max.\\n\"\n",
        "  \"Based on this, tell me 1 really cool fact about this product.\\n\\n\"\n",
        "\n",
        ")\n",
        "\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key= \"Recommendation\"\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "0fb0730e",
      "metadata": {
        "id": "0fb0730e"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are social media manager.\\n\"\n",
        "    \"This is my product recommendation:\\n{Recommendation}\\n\\n\"\n",
        "    \"Give me three ideas, how to hype this on Instagram.\\n\"\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"On_Social_Media\"\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "6accf92d",
      "metadata": {
        "id": "6accf92d"
      },
      "outputs": [],
      "source": [
        "# prompt template 3: translate to english or other language\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to German:\\n\\n\"\n",
        "    \"{On_Social_Media}\\n\\nGerman Translation:\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"Translation\"\n",
        "                      )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "c7a46121",
      "metadata": {
        "id": "c7a46121"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prompt template 4: follow up message that take as inputs the two previous prompts' variables\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Write a friendly and professional follow-up message to a customer based on the summary and translation review.\"\n",
        "        \"this recommendation and these marketing ideas.n\\n\"\n",
        "        \"Recommendation:\\n{Recommendation}\\n\\n\"\n",
        "        \"Social media ideas:\\n{On_Social_Media}\\n\\n\"\n",
        "        \"German version:\\n{Translation}\\n\\n\"\n",
        "        \"Follow-up message:\"\n",
        ")\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"Follow_up\"\n",
        "                     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "89603117",
      "metadata": {
        "id": "89603117"
      },
      "outputs": [],
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"product_description\"],\n",
        "    output_variables=[\"Recommendation\", \"On_Social_Media\", \"Translation\", \"Follow_up\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "51b04f45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51b04f45",
        "outputId": "1dd2d607-ec1c-4a3b-b895-d80c373e5e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_description': \"Je trouve le go√ªt m√©diocre. La mousse ne tient pas, c'est bizarre. J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...\\r\\nVieux lot ou contrefa√ßon !?\",\n",
              " 'Recommendation': 'The product received negative reviews regarding its taste and foam quality, with some speculating it may be an old batch or counterfeit. \\n\\nOne cool fact about this product is that it is commonly found in stores and has a reputation for having a much better taste than what was experienced by the reviewer.',\n",
              " 'On_Social_Media': '1. Host a giveaway: Create a visually appealing post showcasing the product and its positive attributes, while also addressing the negative reviews in a transparent and honest manner. Encourage followers to participate in a giveaway where they can win a free sample of the product to try for themselves.\\n\\n2. Collaborate with influencers: Partner with popular food and beverage influencers who can create visually engaging content featuring the product. Have them share their own positive experiences with the product, and encourage their followers to give it a try as well.\\n\\n3. Create a user-generated content campaign: Start a hashtag campaign where followers can share their own experiences with the product. Encourage them to post photos or videos of themselves enjoying the product, along with their honest opinions. This will help generate buzz and positive word-of-mouth around the product.',\n",
              " 'Translation': '1. Veranstalten Sie ein Giveaway: Erstellen Sie einen visuell ansprechenden Beitrag, der das Produkt und seine positiven Eigenschaften zeigt, w√§hrend Sie auch transparent und ehrlich mit negativen Bewertungen umgehen. Ermutigen Sie Follower, an einem Giveaway teilzunehmen, bei dem sie eine kostenlose Produktprobe gewinnen k√∂nnen, um es selbst auszuprobieren.\\n\\n2. Arbeiten Sie mit Influencern zusammen: Kooperieren Sie mit beliebten Food- und Getr√§nke-Influencern, die visuell ansprechende Inhalte mit dem Produkt erstellen k√∂nnen. Lassen Sie sie ihre eigenen positiven Erfahrungen mit dem Produkt teilen und ermutigen Sie ihre Follower, es auch auszuprobieren.\\n\\n3. Starten Sie eine Kampagne mit nutzergenerierten Inhalten: Beginnen Sie eine Hashtag-Kampagne, bei der Follower ihre eigenen Erfahrungen mit dem Produkt teilen k√∂nnen. Ermutigen Sie sie, Fotos oder Videos von sich selbst beim Genie√üen des Produkts sowie ihre ehrlichen Meinungen zu ver√∂ffentlichen. Dies wird dazu beitragen, Aufmerksamkeit zu erregen und positive Mundpropaganda √ºber das Produkt zu generieren.',\n",
              " 'Follow_up': 'Dear valued customer,\\n\\nI wanted to reach out to you regarding the recent reviews we received about our product. We are sorry to hear about the negative feedback regarding the taste and foam quality, and we are taking steps to address these concerns. \\n\\nWe want to assure you that the product you received is not an old batch or counterfeit. In fact, it is commonly found in stores and has a reputation for having a much better taste than what you experienced. \\n\\nIn light of this feedback, we have come up with some marketing ideas to showcase the positive attributes of our product and address any concerns raised by customers like yourself. We will be hosting a giveaway, collaborating with influencers, and starting a user-generated content campaign to generate positive buzz around the product. \\n\\nWe appreciate your feedback and take it seriously. Please feel free to reach out to us if you have any further questions or concerns. Thank you for being a loyal customer and we hope to provide you with a better experience in the future.\\n\\nBest regards,\\n[Your Name]'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "review = df.Review[5]\n",
        "overall_chain(review)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3187cf07-458a-4226-bec7-3dec7ee47af2",
      "metadata": {
        "id": "3187cf07-458a-4226-bec7-3dec7ee47af2"
      },
      "source": [
        "**Repeat the above twice for different products or reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3041ea4c",
      "metadata": {
        "id": "3041ea4c"
      },
      "source": [
        "## Router Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ade83f4f",
      "metadata": {
        "id": "ade83f4f"
      },
      "outputs": [],
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "biology_template = \"\"\"You are an excellent biologist. \\\n",
        "You have a deep understanding of living organisms, \\\n",
        "from the molecular and cellular level to entire ecosystems. \\\n",
        "You are skilled at observing patterns in nature, analyzing biological data, \\\n",
        "and explaining complex processes like evolution, genetics, physiology, and ecology. \\\n",
        "You can clearly communicate how life functions and adapts, \\\n",
        "and you make connections between different biological concepts \\\n",
        "to answer challenging questions.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5f590e9f",
      "metadata": {
        "id": "5f590e9f"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"biology\",\n",
        "        \"description\": \"Good for answering biology questions\",\n",
        "        \"prompt_template\": biology_template\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "31b06fc8",
      "metadata": {
        "id": "31b06fc8"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f50bcc",
      "metadata": {
        "id": "f3f50bcc"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "8eefec24",
      "metadata": {
        "id": "8eefec24"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9f98018a",
      "metadata": {
        "id": "9f98018a"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "11b2e2ba",
      "metadata": {
        "id": "11b2e2ba"
      },
      "outputs": [],
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1387109d",
      "metadata": {
        "id": "1387109d"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "2fb7d560",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fb7d560",
        "outputId": "64fe53d3-40ad-49ab-cb06-cb1615bf6038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3038952769.py:1: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
            "  chain = MultiPromptChain(router_chain=router_chain,\n"
          ]
        }
      ],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d86b2131",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "d86b2131",
        "outputId": "7b35ff4a-51c8-4cd5-87dd-5c4d891e13c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which absorbs all incident electromagnetic radiation and emits radiation in all directions. The spectral distribution of black body radiation follows Planck's law, which describes how the intensity of radiation emitted by a black body at a given wavelength or frequency is dependent on its temperature. This phenomenon is important in understanding the behavior of objects at high temperatures, such as stars, and is a key concept in the field of thermal physics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3b717379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "3b717379",
        "outputId": "0f16a952-6639-479b-e855-e62a9686e9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2 + 2 equals 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "29e5be01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "29e5be01",
        "outputId": "084d35bd-38fb-4d43-c414-7ab663561541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "biology: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for building and regulating all the structures and functions of a living organism. DNA is essential for the growth, development, and maintenance of an organism. It contains the information needed to produce proteins, which are the building blocks of cells and perform a wide variety of functions in the body.\\n\\nAdditionally, DNA contains the genetic code that determines an organism's traits, such as hair color, eye color, and susceptibility to certain diseases. Each cell in our body contains the same DNA, with the exception of sperm and egg cells, which have half the amount of DNA to combine during fertilization to create a new organism.\\n\\nOverall, DNA is crucial for the proper functioning of cells and the continuation of life processes, which is why every cell in our body contains DNA.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e0c60b-7ae0-453e-9467-142d8dafee6e",
      "metadata": {
        "id": "09e0c60b-7ae0-453e-9467-142d8dafee6e"
      },
      "source": [
        "**Repeat the above at least once for different inputs and chains executions - Be creative!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "dea40f2e-f2bf-446c-f937-43b28ea98041",
        "id": "FfH8W3Zs4z26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "biology: {'input': 'How much blood does a woman normally have?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'On average, a woman typically has about 4 to 5 liters of blood in her body. This amount can vary depending on factors such as body size, age, and overall health. Blood volume can also fluctuate during pregnancy or menstruation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "chain.run(\"How much blood does a woman have normally?\")"
      ],
      "id": "FfH8W3Zs4z26"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cb658fa5-51fd-4b4a-9102-a2d67313ac5b",
        "id": "IfXN_W1955xd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "biology: {'input': 'Why do so many people between ages 40-60 get knee problems? How do I prevent that happening too early?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As a biologist, I can provide insights into why knee problems are common among individuals aged 40-60 and suggest ways to prevent them from occurring prematurely.\\n\\nOne of the main reasons why knee problems are prevalent in this age group is due to the natural aging process. As we get older, the cartilage in our joints, including the knee, wears down, leading to conditions such as osteoarthritis. Additionally, factors such as previous injuries, obesity, genetics, and a sedentary lifestyle can also contribute to an increased risk of knee problems.\\n\\nTo prevent knee problems from occurring too early, it is important to take care of your knee health throughout your life. This includes:\\n\\n1. Maintaining a healthy weight: Excess weight puts extra stress on your knees, increasing the risk of developing knee problems. Maintaining a healthy weight through a balanced diet and regular exercise can help reduce this risk.\\n\\n2. Staying active: Regular exercise, particularly activities that strengthen the muscles around the knee, can help support and protect the joint. Low-impact exercises like swimming, biking, and walking are often recommended for knee health.\\n\\n3. Avoiding high-impact activities: Activities that put a lot of stress on the knees, such as running on hard surfaces or participating in contact sports, can increase the risk of knee injuries. Consider choosing lower-impact alternatives to protect your knees.\\n\\n4. Practicing good posture: Maintaining proper posture can help distribute weight evenly across your joints, reducing strain on the knees.\\n\\n5. Listening to your body: Pay attention to any pain or discomfort in your knees, as this may be a sign of an underlying issue. If you experience persistent knee pain, it is important to consult a healthcare professional for a proper diagnosis and treatment plan.\\n\\nBy taking proactive steps to care for your knee health, you can help prevent knee problems from occurring prematurely and maintain optimal mobility as you age.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "chain.run(\"Why do so many people between ages 40-60 get knee problems? How do I prevent that happening too early?\")"
      ],
      "id": "IfXN_W1955xd"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "65e75fcf-0efd-40a5-8664-bc033eddecd5",
        "id": "evdlTO2b5Cmr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Do aliens exist?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There is currently no conclusive evidence that proves the existence of aliens. While many scientists believe that it is statistically likely that intelligent life exists elsewhere in the universe given the vast number of planets and galaxies, no direct evidence of extraterrestrial life has been found. The search for extraterrestrial life continues through programs like SETI (Search for Extraterrestrial Intelligence) and missions to Mars and other planets that could potentially support life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "chain.run(\"Do aliens exist?\")"
      ],
      "id": "evdlTO2b5Cmr"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "e1069a61-ceef-4a69-ec4c-6580b7a7ada2",
        "id": "OCSydcM56hae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'What age am I?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I cannot determine your age as I do not have access to personal information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "chain.run(\"What age am I?\")"
      ],
      "id": "OCSydcM56hae"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}